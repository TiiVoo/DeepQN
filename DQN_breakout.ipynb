{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17530,"status":"ok","timestamp":1684002171000,"user":{"displayName":"Timo VÃ¶lkl","userId":"16036641020473707307"},"user_tz":-120},"id":"oF6ECzijQi-X","outputId":"1352a64d-bae1-4400-f7c7-c911a8ce9b34"},"outputs":[],"source":["\n","!pip install gym[atari,accept-rom-license]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"HoH-twmyUaJ-"},"outputs":[],"source":["import numpy as np\n","import gym\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Conv2D, Flatten\n","from tensorflow.keras.optimizers import Adam\n","from collections import deque\n","import random\n","from PIL import Image\n","import pickle\n","from skimage.color import rgb2gray\n","from skimage.transform import resize\n","\n","MODEL_PATH = \"./drive/MyDrive/Colab Notebooks/dQN_atari_model\"\n","IM_SIZE = 84\n","AGENT_HISTORY = 4"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"5U7MfGrez6UU"},"outputs":[],"source":["def plot_running_avg(totalrewards):\n","    N = len(totalrewards)\n","    running_avg = np.empty(N)\n","    for t in range(N):\n","        running_avg[t] = totalrewards[max(0, t - 100):(t + 1)].mean()\n","    plt.plot(running_avg)\n","    plt.title(\"Running Average\")\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ILWZBRHYQze0"},"outputs":[],"source":["def preprocess_frame(frame):\n","    frame = frame[31:194, 8:152]\n","    frame = tf.image.rgb_to_grayscale(frame)\n","    frame = tf.image.resize(frame, (IM_SIZE, IM_SIZE), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n","    frame = tf.squeeze(frame)\n","    frame = tf.cast(frame, tf.uint8)\n","    return frame\n","    \n","\n","\n","class ReplayMemory:\n","    def __init__(self, size, batch_size, frame_height=IM_SIZE, frame_width=IM_SIZE,\n","                 agent_history_length=AGENT_HISTORY):\n","        \"\"\"\n","        Args:\n","            size: Integer, Number of stored transitions\n","            frame_height: Integer, Height of a frame of an Atari game\n","            frame_width: Integer, Width of a frame of an Atari game\n","            agent_history_length: Integer, Number of frames stacked together to create a state\n","            batch_size: Integer, Number of transitions returned in a minibatch\n","        \"\"\"\n","        self.size = size\n","        self.frame_height = frame_height\n","        self.frame_width = frame_width\n","        self.agent_history_length = agent_history_length\n","        self.batch_size = batch_size\n","        self.count = 0\n","        self.current = 0\n","\n","        # Pre-allocate memory\n","        self.actions = np.empty(self.size, dtype=np.int32)\n","        self.rewards = np.empty(self.size, dtype=np.float32)\n","        self.frames = np.empty((self.size, self.frame_height, self.frame_width), dtype=np.uint8)\n","        self.terminal_flags = np.empty(self.size, dtype=np.bool)\n","\n","        # Pre-allocate memory for the states and new_states in a minibatch\n","        self.states = np.empty((self.batch_size, self.agent_history_length,\n","                                self.frame_height, self.frame_width), dtype=np.uint8)\n","        self.new_states = np.empty((self.batch_size, self.agent_history_length,\n","                                    self.frame_height, self.frame_width), dtype=np.uint8)\n","        self.indices = np.empty(self.batch_size, dtype=np.int32)\n","\n","    def add_experience(self, action, frame, reward, terminal):\n","        \"\"\"\n","        Args:\n","            action: An integer-encoded action\n","            frame: One grayscale frame of the game\n","            reward: reward the agend received for performing an action\n","            terminal: A bool stating whether the episode terminated\n","        \"\"\"\n","        if frame.shape != (self.frame_height, self.frame_width):\n","            raise ValueError('Dimension of frame is wrong!')\n","        self.actions[self.current] = action\n","        self.frames[self.current, ...] = frame\n","        self.rewards[self.current] = reward\n","        self.terminal_flags[self.current] = terminal\n","        self.count = max(self.count, self.current + 1)\n","        self.current = (self.current + 1) % self.size\n","\n","    def _get_state(self, index):\n","        if self.count == 0:\n","            raise ValueError(\"The replay memory is empty!\")\n","        if index < self.agent_history_length - 1:\n","            raise ValueError(\"Index must be min 3\")\n","        return self.frames[index - self.agent_history_length + 1:index + 1, ...]\n","\n","    def _get_valid_indices(self):\n","        for i in range(self.batch_size):\n","            while True:\n","                index = random.randint(self.agent_history_length, self.count - 1)\n","                if index < self.agent_history_length:\n","                    continue\n","                if index >= self.current and index - self.agent_history_length <= self.current:\n","                    continue\n","                if self.terminal_flags[index - self.agent_history_length:index].any():\n","                    continue\n","                break\n","            self.indices[i] = index\n","\n","    def get_minibatch(self):\n","        \"\"\"\n","        Returns a minibatch of self.batch_size transitions\n","        \"\"\"\n","        if self.count < self.agent_history_length:\n","            raise ValueError('Not enough memories to get a minibatch')\n","\n","        self._get_valid_indices()\n","\n","        for i, idx in enumerate(self.indices):\n","            self.states[i] = self._get_state(idx - 1)\n","            self.new_states[i] = self._get_state(idx)\n","\n","        return np.transpose(self.states, axes=(0, 2, 3, 1)), self.actions[self.indices], self.rewards[\n","            self.indices], np.transpose(self.new_states, axes=(0, 2, 3, 1)), self.terminal_flags[self.indices]\n","\n","\n","\n","class DQGModel:\n","    def __init__(self, state_shape, n_actions, memory_size=1000000, min_memory=1000000,min_exploration=10000, batch_size=5000, gamma=0.99, epsilon=1,\n","                 epsilon_min=0.01, epsilon_decay=(0.9), learning_rate=0.00025,update_target_freq=100):\n","        self.state_shape = state_shape\n","        self.n_actions = n_actions\n","        self.memory = ReplayMemory(memory_size,batch_size)\n","        self.batch_size = batch_size\n","        self.gamma = gamma\n","        self.epsilon = epsilon\n","        self.epsilon_min = epsilon_min\n","        self.epsilon_decay = epsilon_decay\n","        self.learning_rate = learning_rate\n","        self.min_memory = min_memory\n","        self._optimizer = tf.keras.optimizers.Adam(learning_rate)\n","        self.model = self._build_model()\n","        self.target_model = self._build_model()\n","        self.update_target_freq = update_target_freq\n","        self.steps_taken = 0\n","        self.min_exploration = min_exploration\n","        \n","\n","     \n","    def _build_model(self):\n","        model = Sequential()\n","        model.add(Conv2D(32, (8, 8), strides = 4, activation='relu', input_shape=self.state_shape))\n","        model.add(Conv2D(64, (4, 4), strides= 2, activation='relu'))\n","        model.add(Conv2D(64, (3, 3), strides= 1, activation='relu'))\n","        model.add(Flatten())\n","        model.add(Dense(512, activation='relu'))\n","        model.add(Dense(self.n_actions, activation='linear'))\n","        model.compile(loss=tf.keras.losses.Huber(delta=1.0), optimizer=self._optimizer)\n","        return model\n","\n","    def remember(self, action, next_frame, reward, done):\n","        self.memory.add_experience(action, next_frame, reward, done)\n","    \n","    def act(self, state):\n","        if np.random.rand() <= self.epsilon:\n","            return random.randrange(self.n_actions)\n","        q_values = self.model(state)\n","        return tf.argmax(q_values[0])\n","\n","    def replay(self):\n","        if self.memory.count < self.batch_size or self.memory.count < self.min_memory:\n","            return\n","\n","        states, actions, rewards, next_states, dones = self.memory.get_minibatch()\n","        \n","        with tf.GradientTape() as tape:\n","            q_values = self.model(states)\n","            target = tf.identity(q_values)\n","            updates = rewards + (1 - tf.cast(dones, tf.float32)) * self.gamma * tf.reduce_max(\n","                self.target_model(next_states),axis=1)\n","            indices = tf.stack([tf.range(self.batch_size), actions], axis=-1)\n","            target = tf.tensor_scatter_nd_update(target, indices, updates)\n","            loss = tf.keras.losses.Huber(delta=1.0)(target, q_values)\n","\n","        grads = tape.gradient(loss, self.model.trainable_variables)\n","        self._optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n","\n","        self.steps_taken += 1\n","\n","        if self.epsilon > self.epsilon_min and self.steps_taken > self.min_exploration:\n","            self.epsilon -= self.epsilon_decay/self.min_exploration\n","\n","        \n","        if self.steps_taken % self.update_target_freq == 0:\n","            self.update_target_network()\n","\n","    def update_target_network(self):\n","        self.target_model.set_weights(self.model.get_weights())\n","    \n","    def save_model(self, filepath):\n","        self.model.save((filepath+ \".h5\"))\n","        self.target_model.save((filepath+ \"tg.h5\"))\n","\n","        with open((filepath+'mem.pkl'), 'wb') as file:\n","            pickle.dump(self.memory, file)\n","        with open((filepath+'param.pkl'), 'wb') as file:\n","            pickle.dump((self.steps_taken, self.epsilon), file)\n","    def load_model(self, filepath):\n","        self.model = tf.keras.models.load_model((filepath+\".h5\"))\n","        self.target_model = tf.keras.models.load_model((filepath+\"tg.h5\"))\n","        with open((filepath+\"param.pkl\"), 'rb') as file:\n","            self.steps_taken, self.epsilon = pickle.load(file)   \n","    def load_memory(self, filepath):\n","        with open((filepath+\"mem.pkl\"), 'rb') as file:\n","            self.memory = pickle.load(file)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"3o359WDlQ2zc"},"outputs":[],"source":["\n","\n","def run_episode(env,model):\n","    frame = env.reset()\n","    frame = preprocess_frame(frame)\n","    state = np.stack([frame] * 4, axis=2)\n","    state = np.expand_dims(state, axis=0)\n","    max_iter = 200000\n","    count = 0\n","    total_reward = 0\n","\n","    done = False\n","\n","    while not done and count < max_iter:\n","\n","        action = model.act(state)  # get action\n","        [next_frame, reward, done, _] = env.step(action)  # get new state\n","        #plt.imshow(next_frame) plots the episode in the notebook \n","        #plt.show()\n","\n","\n","        next_frame = preprocess_frame(next_frame)\n","        model.remember(action, next_frame, reward, done)\n","        next_frame = np.expand_dims(next_frame, axis=2)\n","        next_frame = np.expand_dims(next_frame, axis=0)\n","        next_state = np.append(state[:, :, :, 1:], next_frame, axis=3)\n","\n","        total_reward += reward\n","        count += 1\n","        state = next_state\n","    model.replay()\n","\n","    return total_reward\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EptmjsaACKZw"},"outputs":[],"source":["env = gym.make('BreakoutDeterministic-v4')\n","state_shape = (IM_SIZE, IM_SIZE, AGENT_HISTORY) # 84x84 after rescaling and 4 steps history\n","n_actions = env.action_space.n\n","\n","model = DQGModel(state_shape, n_actions)\n","model.load_model(MODEL_PATH)\n","model.load_memory(MODEL_PATH)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RcWNf14HQ6Bl"},"outputs":[],"source":["\n","n_episodes = 10002\n","total_rewards = np.zeros(n_episodes)\n","for i in range(n_episodes):\n","\n","    total_count = run_episode(env,model)\n","    total_rewards[i] = total_count\n","\n","    plt.plot(total_rewards)\n","    plt.title(\"Rewards\")\n","    plt.show()\n","    print(\"episode:\", i, \"steps:\", model.steps_taken, \"epsilon:\", model.epsilon, \"total reward:\", total_count)\n","    if i%100 == 0 and i >100:\n","      model.save_model(MODEL_PATH)\n","    \n","    if i % 100 == 0:\n","        print(\"avg reward for last 100 episodes:\", total_rewards[-100:].mean())\n","\n","plt.plot(total_rewards)\n","plt.title(\"Rewards\")\n"]},{"cell_type":"markdown","metadata":{"id":"t_xJKvIkEQkl"},"source":["\n","\n","```\n","`# This is formatted as code`\n","```\n","\n","JS to prevent idle timeout:\n","\n","Press F12 OR CTRL + SHIFT + I OR right click on this website -> inspect.\n","Then click on the console tab and paste in the following code.\n","\n","```javascript\n","function ClickConnect(){\n","\n","console.log(\"Working\"); \n","document.querySelector(\"#comments > span\").click() \n","}\n","setInterval(ClickConnect,5000)\n","```"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNT4zZp/X57DE0AOzZEP7lt","machine_shape":"hm","mount_file_id":"1kgAUvK3VWPrOf4XN_FLDAWxXRxFlNR7s","name":"","version":""},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
