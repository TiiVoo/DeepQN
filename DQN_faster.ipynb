{"cells":[{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":859,"status":"ok","timestamp":1678750069853,"user":{"displayName":"Timo Völkl","userId":"16036641020473707307"},"user_tz":-60},"id":"lK473yJ8W93P"},"outputs":[],"source":["\n","\n","\n","import gym\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import tensorflow as tf\n","from collections import deque\n","import random\n","\n"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1678750069854,"user":{"displayName":"Timo Völkl","userId":"16036641020473707307"},"user_tz":-60},"id":"8hdijdHxdkvn"},"outputs":[],"source":["def plot_running_avg(totalrewards):\n","    N = len(totalrewards)\n","    running_avg = np.empty(N)\n","    for t in range(N):\n","        running_avg[t] = totalrewards[max(0, t - 100):(t + 1)].mean()\n","    plt.plot(running_avg)\n","    plt.title(\"Running Average\")\n","    plt.show()\n"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1678750069855,"user":{"displayName":"Timo Völkl","userId":"16036641020473707307"},"user_tz":-60},"id":"IaE44VUwbPVW"},"outputs":[],"source":["\n","class DQG_model:\n","    def __init__(self, env):\n","\n","        self.env = env\n","        # Initialize attributes\n","        self._state_size = env.observation_space.shape[0]\n","        self._action_size = env.action_space.n\n","\n","        self.experience_replay = deque(maxlen=200000)\n","\n","        self._optimizer = tf.keras.optimizers.Adam(learning_rate=0.005)\n","\n","        # Initialize discount and exploration rate\n","        self.gamma = 0.99\n","        self.epsilon = 0.1\n","\n","        # Build networks\n","        self.q_network = self._build_compile_model()\n","        self.target_network = self._build_compile_model()\n","        self.alighn_target_model()\n","\n","    def store(self, state, action, reward, next_state, done):\n","        self.experience_replay.append((state, action, reward, next_state, done))\n","\n","    def _build_compile_model(self):\n","        model = tf.keras.Sequential()\n","        model.add(tf.keras.layers.Dense(20, input_dim=self._state_size, activation='relu'))\n","        model.add(tf.keras.layers.Dense(50, input_dim=self._state_size, activation='relu'))\n","\n","        model.add(tf.keras.layers.Dense(self._action_size, activation='linear'))\n","\n","        model.compile(loss='mse', optimizer=self._optimizer)\n","\n","        return model\n","\n","    def alighn_target_model(self):\n","        self.target_network.set_weights(self.q_network.get_weights())\n","\n","    @tf.function\n","    def act(self, state):\n","        if tf.random.uniform([]) <= self.epsilon:\n","            return tf.random.uniform([], minval=0, maxval=self._action_size, dtype=tf.int64)\n","\n","        q_values = self.q_network(state)\n","        return tf.argmax(q_values[0])\n","\n","\n","    def retrain(self, batch_size):\n","        minibatch = random.sample(self.experience_replay, batch_size)\n","        states, actions, rewards, next_states, dones = zip(*minibatch)\n","\n","        with tf.GradientTape() as tape:\n","            q_values = self.q_network(tf.concat(states, axis=0))\n","\n","            target = tf.identity(q_values)\n","            updates = rewards + (1 - tf.cast(dones, tf.float32)) * self.gamma * tf.reduce_max(\n","                self.target_network(tf.concat(next_states, axis=0)), axis=-1)\n","            indices = tf.stack([tf.range(batch_size), actions], axis=-1)\n","            target = tf.tensor_scatter_nd_update(target, indices, updates)\n","\n","            loss = tf.keras.losses.mean_squared_error(target, q_values)\n","\n","        grads = tape.gradient(loss, self.q_network.trainable_variables)\n","        self._optimizer.apply_gradients(zip(grads, self.q_network.trainable_variables))\n"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1678750069855,"user":{"displayName":"Timo Völkl","userId":"16036641020473707307"},"user_tz":-60},"id":"7PtLtPNPbU3v"},"outputs":[],"source":["\n","def run_episode(env,model):\n","    state = env.reset()\n","    state = state.reshape((1, -1))\n","    max_iter = 2000\n","    count = 0\n","    totalreward = 0\n","    batch_size = 2000\n","\n","    done = False\n","    while not done and count < max_iter:\n","        if debug: print(\"count =\", count)\n","        action = model.act(state)  # get action\n","        action = action.numpy()\n","        [next_state, reward, done, _] = env.step(action)  # get new state\n","        next_state = next_state.reshape((1, -1))\n","\n","        if done:\n","            reward = -200\n","\n","        model.store(state, action, reward, next_state, done)\n","\n","        totalreward += reward\n","\n","        if len(model.experience_replay)  > batch_size:\n","            model.retrain(batch_size)\n","            \n","\n","\n","        count += 1\n","        state = next_state\n","    \n","    model.alighn_target_model()\n","    \n","\n","    return totalreward\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":325,"status":"ok","timestamp":1678750088467,"user":{"displayName":"Timo Völkl","userId":"16036641020473707307"},"user_tz":-60},"id":"UeQFsJmt5DGz","outputId":"bf41b515-4ffd-4c3e-938c-da8c6270fc20"},"outputs":[],"source":["debug=0\n","env = gym.make('CartPole-v1')\n","\n","model = DQG_model(env)\n","model.q_network.summary()\n","# Create the optimizer\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1532310,"status":"ok","timestamp":1678752340950,"user":{"displayName":"Timo Völkl","userId":"16036641020473707307"},"user_tz":-60},"id":"53xx242D5JYX","outputId":"9bdaae9e-469a-4c90-c704-e8a7de3b51b1"},"outputs":[],"source":["\n","n_episodes = 50\n","total_rewards = np.empty(n_episodes)\n","for i in range(n_episodes):\n","\n","    total_count = run_episode(env,model)\n","    total_rewards[i] = total_count\n","    plt.plot(total_rewards)\n","    plt.title(\"Rewards\")\n","    plt.show()\n","\n","\n","    print(\"episode:\", i, \"total reward:\", total_count)\n","print(\"avg reward for last 100 episodes:\", total_rewards[-100:].mean())\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOo6txy4aeucf/rUYCrK1iO","provenance":[{"file_id":"1RD1qErgPEm4uNYJi_7OZP18lM1oZVEbp","timestamp":1678745075278}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
